# @title Google AI Studio Proxy for JanitorAI

# @markdown # **GeminiForJanitors** (August 1, 2025) by `undefinedundefined`

# @markdown Press the ‚èµ button on the left to run this cell and start the proxy server, then copy the generated `trycloudflare.com` link to use in JanitorAI.

# @markdown You might have to press `Save Changes` twice on Edit Configuration, then `Save Settings` on Proxy API Configurations and reload the page, before you can use the proxy with a new link.

# @markdown Press the `Test` button on JanitorAI to check if the proxy is working for you. If you get a `Network error. Try again later!` message, try the above to get it working.

!pip install -q flask flask_cloudflared flask_cors requests waitress

import json
import requests
import time
import threading
import traceback
from flask import Flask, Response, request
from flask_cloudflared import start_cloudflared
from flask_cors import CORS
from functools import wraps
from waitress import serve

################################################################################

# @markdown ## Safety Settings (i.e. whether you can have NSFW or not)

# @markdown Gemini allows to you "controls the probability threshold at which harm is blocked," which is to say, you can specify how spicy your chat can get before the model refuses to talk to you anymore.

# @markdown Your chat is measured by how "harmful" it is and, if above the specified threshold, responses from the model will be blocked and you will get an error. From most to least restrictive, these thresholds are:

# @markdown - `BLOCK_LOW_AND_ABOVE`: Content with NEGLIGIBLE will be allowed.

# @markdown - `BLOCK_MEDIUM_AND_ABOVE`: Content with NEGLIGIBLE and LOW will be allowed.

# @markdown - `BLOCK_ONLY_HIGH`: Content with NEGLIGIBLE, LOW, and MEDIUM will be allowed.

# @markdown - `BLOCK_NONE`: All content will be allowed.

# @markdown - `OFF`: Turn off the safety filter. (This is documented to exist but its exact difference with `BLOCK_NONE` is not.)

# @markdown Here you can configure the global safety threshold for your proxy instance, affecting all chats on proxy on all kinds of content ("harassment," "hate speech," "sexually explicit," "dangerous," and "civic integrity"):

SAFETY_SETTINGS_THRESHOLD = 'OFF' # @param [ 'OFF', 'BLOCK_NONE', 'BLOCK_ONLY_HIGH', 'BLOCK_MEDIUM_AND_ABOVE', 'BLOCK_LOW_AND_ABOVE' ]

SAFETY_SETTINGS = [
    {
        'category': 'HARM_CATEGORY_HARASSMENT',
        'threshold': SAFETY_SETTINGS_THRESHOLD,
    },
    {
        'category': 'HARM_CATEGORY_HATE_SPEECH',
        'threshold': SAFETY_SETTINGS_THRESHOLD,
    },
    {
        'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT',
        'threshold': SAFETY_SETTINGS_THRESHOLD,
    },
    {
        'category': 'HARM_CATEGORY_DANGEROUS_CONTENT',
        'threshold': SAFETY_SETTINGS_THRESHOLD,
    },
    {
        'category': 'HARM_CATEGORY_CIVIC_INTEGRITY',
        'threshold': SAFETY_SETTINGS_THRESHOLD,
    },
]

################################################################################

# @markdown ## Generation Settings (i.e. how should the model's output be generated)

# @markdown These are more technical settings that can have significant impact on the model's quality and performance. The default values are arbitrary but may work good enough for most cases. Note how JanitorAI, at the time of writing, doesn't have a mean to configure this stuff, but with a proxy anything is possible. These settings are global and affect all chats on the proxy.

# @markdown Top-K sampling is kinda like how big the selection of words is at any time while writing. Small values will likely cripple the model's ability to write something interesting while large values will likely allow it to write nonsense. Set to zero to disable this setting.

TOP_K = 50 # @param { type: "slider", min: 0, max: 100, step: 1 }

# @markdown Top-P sampling is a more adaptive form of Top-K sampling, which takes into account the likelihood of words when preparing a selection. May or may not be a more meaningful form of the `temperature` parameter. Lower values force may more a "sensible" selection. Set to zero to disable this setting.

TOP_P = 0.95 # @param { type: "slider", min: 0.0, max: 1.0, step: 0.01 }

# @markdown Frequency penalty and presence penalty help prevent repetition on the model. Frequency penalty guides the model away from using the same words multiple times. Presence penalty guides the model towards using new words. Higher values have stronger effects on the output. Set to zero to disable these setting. (Note: I haven't personally tested any of these, so they are disabled by default.)

FREQUENCY_PENALTY = 0.0 # @param { type: "slider", min: 0.0, max: 1.0, step: 0.01 }

PRESENCE_PENALTY = 0.0 # @param { type: "slider", min: 0.0, max: 1.0, step: 0.01 }

# @markdown Before some nerd emoji comes here to says "uhm ackshually," I'll let you I know I am aware that these settings work with *TOKENS* and not with *WORDS*, I just don't care about the difference.

################################################################################

# @markdown ## Miscellaneous Settings (i.e. proxy stuff)

# @markdown Whether to print or not the raw JSON data received from JanitorAI and Google AI, useful if you are a developer working on this trash. **Turning this on will allow you to steal bots with hidden descriptions**, if you know what to look for. I will not be made responsible for that.

DEBUG_PRINT_JSON = False # @param { type: "boolean" }

# @markdown This proxy does not implement streaming (which may increase the number of rejections) and so it might make waiting for the model's responses bothersome. On top of that, being a Chain of Thought model, Gemini might take a while to respond. The default timeout is set to half a minute. Changing this value will not make the model faster, but a short enough time out will give you some (error) feedback whenever the model is taking too long to answer. Consider keeping your chats simple so that the model doesn't have to think too much about it.

REQUEST_TIMEOUT_IN_SECONDS = 30 # @param { type: "integer" }

################################################################################

# JanitorAI user handle
PROXY_AUTHORS = "undefinedundefined"

PROXY_NAME = "GeminiForJanitors"

PROXY_VERSION = "2025.08.01"

# HTML pro-tip: you can always omit <body> and <head> (screw IE9)
# See https://stackoverflow.com/a/5642982
# <html> is optional too, but lang=en is good manners
INDEX = fr"""<!doctype html>
<html lang=en>
<meta charset=utf-8>
<title>Google AI Studio Proxy for JanitorAI</title>
<h1>{PROXY_NAME} ({PROXY_VERSION}) by {PROXY_AUTHORS}</h1>
<p>Up and running.</p>
</html>
"""

SEPARATOR = '=' * 80

SGR_BOLD_ON  = "\x1B[1m"
SGR_BOLD_OFF = "\x1B[22m"

SGR_FORE_DEFAULT = "\x1B[39m"
SGR_FORE_RED     = "\x1B[31m"
SGR_FORE_GREEN   = "\x1B[32m"
SGR_FORE_YELLOW  = "\x1B[33m"
SGR_FORE_BLUE    = "\x1B[34m"

################################################################################

# JanitorAI proxy test request is a single user message with "Just say TEST"
# max_tokens is set to 10 and temperature to 0, but these checks are sufficient
def is_jai_proxy_test(messages):
    return len(messages) == 1 \
        and messages[0].get('role', '') == 'user' \
        and messages[0].get('content', '') == "Just say TEST"

def debug_print_json(obj):
    if DEBUG_PRINT_JSON and obj:
        print(f"{SGR_FORE_YELLOW}{json.dumps(obj, indent=4)}{SGR_FORE_DEFAULT}")

def print_settings():
    print()
    print(end=SGR_BOLD_ON + SGR_FORE_GREEN)
    print(f"{PROXY_NAME} ({PROXY_VERSION}) settings:")
    print(f" * {SAFETY_SETTINGS_THRESHOLD = }")
    print(f" * {TOP_K = }")
    print(f" * {TOP_P = }")
    print(f" * {FREQUENCY_PENALTY = }")
    print(f" * {PRESENCE_PENALTY = }")
    print(f" * {DEBUG_PRINT_JSON = }")
    print(f" * {REQUEST_TIMEOUT_IN_SECONDS = }")
    print(end=SGR_BOLD_OFF + SGR_FORE_DEFAULT)
    print()

def print_timed_message(message, prev_ref_time=None):
    current_ref_time = time.monotonic()

    delta_time_msg = ""
    if isinstance(prev_ref_time, float):
        delta_time_msg = f" ({current_ref_time - prev_ref_time:.0f} seconds)"

    print(f"{SGR_FORE_BLUE}[{time.strftime('%Y-%m-%d %H:%M:%S')}] {message}{delta_time_msg}{SGR_FORE_DEFAULT}")

    return current_ref_time

# This is all JanitorAI seems to need to present a custom error message.
def error_message(message):
    return { 'error': message }

# For pretty output
def proxy_wrapper(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        print()
        print(f"{SGR_BOLD_ON}{SEPARATOR}{SGR_BOLD_OFF}")

        ref_time = print_timed_message("Processing request...")

        try:
            response, status = func(*args, **kwargs)
        except Exception as e:
            response, status = (error_message("Internal Proxy Error"), 500)

            print(end=SGR_FORE_RED)
            print(f"Unexpected error in {func.__name__} handler.")
            traceback.print_exception(e)
            print(end=SGR_FORE_DEFAULT)

        if 200 <= status <= 299:
            print_timed_message("Processing succeeded", ref_time)
        else:
            print_timed_message(f"Processing failed.", ref_time)
            print(f"{SGR_FORE_RED}{status} {response['error']}{SGR_FORE_DEFAULT}")

        print(f"{SGR_BOLD_ON}{SEPARATOR}{SGR_BOLD_OFF}")

        return Response(
            response=json.dumps(response),
            status=status,
            mimetype='text/json')

    return wrapper

################################################################################

app = Flask(__name__)
CORS(app) # JanitorAI requires CORS on proxies

@app.route('/', methods=['GET'])
def index():
    return Response(
        response=INDEX,
        status=200,
        mimetype='text/html')

@app.route('/', methods=['POST'])
@app.route('/chat/completions', methods=['POST'])
@proxy_wrapper
def proxy():
    request_json = request.get_json(silent=True)
    if not request_json:
        return error_message("Bad Request. Missing or invalid JSON from JanitorAI."), 400
    debug_print_json(request_json)


    # JanitorAI provides the user's API key through HTTP Bearer authentication.
    # Google AI cannot be used without an API key.

    request_auth = request.headers.get('authorization')
    if not request_auth or not request_auth.startswith('Bearer '):
        return error_message("Unauthorized. API key required."), 401

    jai_api_key = request_auth[len('Bearer '):]


    # Streaming isn't and won't be implemented as it could increase rejections
    if request_json.get('stream', False):
        return error_message("Text streaming is not supported. Disable text streaming to use this proxy."), 501


    # JanitorAI messages are a list of { 'content': '...', 'role': '...' }
    # Messages are in chronological order, from oldest to newest.
    # Roles can be 'system', 'user', or 'assistant'.
    # There is system message at the beginning with the bot specification.
    # Proxy test has no system message and only has a single user message.

    jai_messages = request_json.get('messages', [])


    # JanitorAI allows the user to configure max_tokens, model and temperature.
    # There is currently no means to configure other model parameters.
    # The code can manage if they come to exist and gracefully default if not.

    jai_max_tokens  = request_json.get('max_tokens', 0)
    jai_model       = request_json.get('model', 'gemini-2.5-pro')
    jai_temperature = request_json.get('temperature', 0)
    jai_top_k       = request_json.get('top_k', TOP_K)
    jai_top_p       = request_json.get('top_p', TOP_P)
    jai_frequency_p = request_json.get('frequency_penalty', FREQUENCY_PENALTY)
    jai_presence_p  = request_json.get('presence_penalty', PRESENCE_PENALTY)


    # JanitorAI message format needs to be converted to Google AI

    gem_system_prompt       = ''
    gem_chat_prompt_content = []
    gem_chat_prompt_length  = 0

    for msg in jai_messages:
        msg_content = msg.get('content', '<|empty|>')
        msg_role    = msg.get('role', 'user')

        if msg_role == 'assistant':
            msg_role = 'model' # Google AI convention

        if msg_role == 'system':
            gem_system_prompt = msg_content
        else:
            gem_chat_prompt_length += len(msg_content)
            gem_chat_prompt_content.append({
                'parts': [{
                    'text': msg_content
                }],
                'role': msg_role,
            })


    # Google AI generation configuration

    gem_config = {
        'temperature': jai_temperature,
        'thinkingConfig': {
            'includeThoughts': True,
        },
    }

    if jai_top_k > 0:
        gem_config['topK'] = jai_top_k

    if jai_top_p > 0.0:
        gem_config['topP'] = jai_top_p

    if jai_frequency_p > 0.0:
        gem_config['frequencyPenalty'] = jai_frequency_p

    if jai_presence_p > 0.0:
        gem_config['presencePenalty'] = jai_presence_p

    # JanitorAI proxy test request has a very low max_tokens value.
    # It is not enough to accommodate thinking tokens, leading to rejections.
    # Otherwise, honor a max_tokens limit. Users will get an error on rejection.

    if jai_max_tokens > 0 and not is_jai_proxy_test(jai_messages):
        gem_config['maxOutputTokens'] = jai_max_tokens


    # Let's get this bread

    print(end="Sending request to Google AI...", flush=True)

    response = requests.post(
        f'https://generativelanguage.googleapis.com/v1beta/models/{jai_model}:generateContent',
        json={
            'contents': gem_chat_prompt_content,
            'systemInstruction': {
                'parts': [{'text': gem_system_prompt}]
            },
            'safetySettings': SAFETY_SETTINGS,
            'generationConfig': gem_config,
        },
        headers={
            'User-Agent': f'{PROXY_NAME}/{PROXY_VERSION}',
            'Content-Type': 'application/json',
            'X-goog-api-key': jai_api_key,
        },
        timeout=REQUEST_TIMEOUT_IN_SECONDS
    )

    print(end=f" {response.status_code} {response.reason}\n", flush=True)

    response_json = response.json()
    if not response_json:
        return error_message("Bad Gateway. Missing or invalid JSON from Google AI."), 502
    debug_print_json(response_json)

    if not response.ok:
        message = "Google AI error: " + response_json.get('error', {}).get('message', "unknown")
        print(message)
        return error_message(message), response.status_code


    # Be as lenient as possible while processing Google AI responses

    gem_candidates = response_json.get('candidates')
    gem_canditate  = gem_candidates[0] if gem_candidates else None
    gem_metadata   = response_json.get('usageMetadata', {})
    gem_feedback   = response_json.get('promptFeedback', {})

    if not isinstance(gem_canditate, dict):
        # TODO: test this code path
        block_reason  = gem_feedback.get('blockReason', 'UNKNOWN')
        block_message = gem_feedback.get('blockReasonMessage', '.')
        print(f"None or null first candidate response. {block_reason = }")
        print(block_message)
        return error_message(f"Response blocked (reason: {block_reason}). Adjust safety settings."), 502

    gem_chat_response = ''
    gem_chat_thinking = ''

    for part in gem_canditate.get('content', {}).get('parts', [{}]):
        part_text = part.get('text')
        part_thought = part.get('thought', False)

        if not part_thought:
            gem_chat_response += part_text
        else:
            gem_chat_thinking += part_text

    gem_chat_response = gem_chat_response.strip()
    gem_chat_thinking = gem_chat_thinking.strip()


    gem_chat_prompt_tokens   = gem_metadata.get('promptTokenCount', 0)
    gem_chat_response_tokens = gem_metadata.get('candidatesTokenCount', 0)
    gem_chat_thinking_tokens = gem_metadata.get('thoughtsTokenCount', 0)
    gem_finish_reason        = gem_canditate.get('finishReason', 'STOP')


    print(f"Chat/Prompt length {gem_chat_prompt_length} tokens {gem_chat_prompt_tokens}.")
    print(f"Response length {len(gem_chat_response)} tokens {gem_chat_response_tokens}.")
    print(f"Thinking length {len(gem_chat_thinking)} tokens {gem_chat_thinking_tokens}:")

    if gem_chat_thinking:
        print(f"{SGR_BOLD_ON}{gem_chat_thinking}{SGR_BOLD_OFF}")

    if gem_finish_reason == 'MAX_TOKENS':
        return error_message("Max tokens exceeded. Increase or remove token limit."), 502


    # All done and good

    return {
        'choices': [{
            'index': 0,
            'message': {
                'role': 'assistant',
                'content': gem_chat_response,
            },
            'finish_reason': 'stop'
        }]
    }, 200

################################################################################

if __name__ == '__main__':
    cloudflared_thread = threading.Thread(
        target=start_cloudflared,
        kwargs={ 'port': 5000, 'metrics_port': 5001, },
        daemon=True)

    cloudflared_thread.start()

    print_settings()

    serve(app, host='0.0.0.0', port=5000)
